# For autoencoder latent space representation
# 1. READ LATENT SPACE DATA
# Specify the path to your latent space CSV file
latent_data_file <- "D:/Desktop/Work/Multiple Lifelines/latent_representations_14lifelines.csv"
# Read the CSV file
latent_df <- read.csv(latent_data_file, header = TRUE, sep = ",", check.names = FALSE)
# 2. RANDOM SHUFFLE
# This step ensures that you're working with a random arrangement of your samples, which can be useful for subsequent analyses.
iniSeed <- 1234
set.seed(iniSeed)
selected <- sample(nrow(latent_df))
latent_df_random <- latent_df[selected,]
# 3. CLUSTERING
maxClus <- 10
eval.array2d <- array(0, dim = c(maxClus - 1, 3),
dimnames = list(as.character(seq(2, maxClus, 1)), c('SI', 'PS', 'Jaccard')))
# Convert dataframe to matrix and ensure all entries are numeric
latent_matrix <- as.matrix(latent_df_random)
latent_matrix <- matrix(as.numeric(unlist(latent_matrix)), nrow=nrow(latent_matrix))
# Silhouette
fitPamBest <- pamk(latent_df_random, krange = 2:maxClus)
library(caret)
library(tidyverse)
library(dplyr)
library(randomForest)
library(fpc) # pamk
library(cluster) # pam
library(ggplot2,quietly=TRUE)
suppressMessages(library(ellipse))
suppressMessages(library(RColorBrewer))
library(reshape2,quietly=TRUE) # Load the reshape2 package (for the melt() function)
source('robust.clustering.metagenomics.functions.r')
#####################################
### Function process.one.fluxes.file
#####################################
# a) To generate subjectID (suffix from file, between 'biomass_' and '.tsv')
# b) To generate sample ID: subjectID_[0*]seqNum
process.one.fluxes.file <- function(file){
suffix = gsub('.csv$','',gsub('^fluxes_','',file))
table=read.table(file,sep="\t",header=TRUE)
table[,grep('time',colnames(table))] <- NULL # Remove repeated time columns (from different strains)
id.subject=rep(suffix,nrow(table))
id.num=seq(1,nrow(table),1)
id.num=str_pad(id.num,5, pad = '0') # Add leading zeros, to easy sort
id.vector=paste(id.subject, id.num,sep = '_')
rownames(table)=id.vector
return(table)
} # end-function process.one.fluxes.file
#####################################
### Function rbind.with.rownames
#####################################
rbind.with.rownames <- function(datalist) {
require(plyr)
temp <- rbind.fill(datalist)
rownames(temp) <- unlist(lapply(datalist, row.names))
return(temp)
} # end-function rbind.with.rownames
#####################################
### Function cluster names
#####################################
cluster_names <- function(df) {
require(dplyr)
means <- df %>%
select(starts_with("Biomass"), cluster) %>%
group_by(cluster) %>%
summarize_all(mean)
signed <- c(as.character(means[which.max(rowSums(means[,2:ncol(means)])),]$cluster),
as.character(means[which.min(rowSums(means[,2:ncol(means)])),]$cluster))
df$cluster <- mapvalues(df$cluster, from = signed, to = c("growth", "stationary"))
return(df)
} # end-function cluster names
#####################################
### Function rf.10CV
#####################################
# Build and evaluate (with 10 Cross-Validation) a model with Random Forest, using caret.
# Args:
#   df: data frame with all data (including predictors and class variable)
#   formula: object of class formula (i.e. class~ predictors (separated by '+'))
#   classVar: string/number of class variable in 'df'. Default: the last one.
#   ntree: number of trees in the forest
# Output:
#   finalModel: object with the model in randomForest format, for further evaluations or plots.
rf.10CV <- function(df,formula,classVar=length(df),ntree=1000){
#require(doMC) # parallel processing; in Windows, maybe doParallel would work
#registerDoMC(cores = 5)
set.seed(123)
train_control <- trainControl(method="cv", number=10, savePredictions = TRUE)
model <- train(form=formula,data=df, trControl=train_control, method="rf", metric='Accuracy',
ntree=ntree, importance=TRUE, localImp=TRUE, na.action=na.omit,
allowParallel = T, ncores = 5, nodesize = 42)
# Define output
output <- list("model"=model$finalModel, "pred"=model$pred)
# summarize results
print(model)
# plot model error
print(plot(model))
# Variable Importance
pdf('varImportance_rf10CV.pdf')
print(varImpPlot(model$finalModel))
dev.off()
print(plot(margin(model$finalModel,df[,classNum])))
return(output)
}
####################################################
### plot.rf.var.importance.by.class.andMean.dotplot
####################################################
# Plot dotplot with variable importance mean over all classes
# Args:
#   model: random forest model already build
#   predVar: string of column ID with predictor/variables names values
#   classVar: string of class variable in 'df'
#   title: header of the plot
#   colorVector: vector of colors
#   nBestFeatures: number of top relevant features to show in the plot.
plot.rf.var.importance.by.class.andMean.dotplot <- function(model,predVar,classVar,title='',colorVector=NULL,nBestFeatures=NULL){
library(reshape2)
imp.df <- melt(importance(model)[,1:(length(model$classes)+1)])
colnames(imp.df)=c(predVar,classVar,'value')
# a.-Order features
pred.order=names(sort(importance(model)[,'MeanDecreaseAccuracy'])) # My order according to global MeandDecreaseAccuracy
imp.df[,predVar] <- factor(imp.df[,predVar], levels = pred.order)
class.names=levels(imp.df[,classVar])
levels(imp.df[,classVar]) <- c(class.names[1:(length(class.names)-1)],"MEAN")
imp.df[,classVar] <- factor(imp.df[,classVar])
# b.- Subset features to show
if(!is.null(nBestFeatures)){
imp.df=subset(imp.df,subset=(imp.df[,predVar] %in% tail(pred.order,n=nBestFeatures)))
}
p <- ggplot(imp.df, aes_string(x = 'value', y = predVar, group = predVar, colour = classVar)) +
geom_segment(aes_string(yend=predVar), xend=0, colour="grey50") +
geom_point( size = 1) +
theme_bw() +
facet_grid(reformulate(classVar)) +
theme(panel.grid.major.y = element_blank()) +
#theme(text = element_text(size=16)) +
xlab(paste(predVar," importance",sep='')) +
theme(axis.text.x = element_text(angle = 270, hjust = 1)) +
theme(legend.position="none") +
ggtitle(title)
if(!is.null(colorVector)){
p +  scale_color_manual(values=colorVector)
}else{
p
}
return(p)
}
# For autoencoder latent space representation
# 1. READ LATENT SPACE DATA
# Specify the path to your latent space CSV file
latent_data_file <- "D:/Desktop/Work/Multiple Lifelines/latent_representations_14lifelines.csv"
# Read the CSV file
latent_df <- read.csv(latent_data_file, header = TRUE, sep = ",", check.names = FALSE)
# 2. RANDOM SHUFFLE
# This step ensures that you're working with a random arrangement of your samples, which can be useful for subsequent analyses.
iniSeed <- 1234
set.seed(iniSeed)
selected <- sample(nrow(latent_df))
latent_df_random <- latent_df[selected,]
# 3. CLUSTERING
maxClus <- 10
eval.array2d <- array(0, dim = c(maxClus - 1, 3),
dimnames = list(as.character(seq(2, maxClus, 1)), c('SI', 'PS', 'Jaccard')))
# Convert dataframe to matrix and ensure all entries are numeric
latent_matrix <- as.matrix(latent_df_random)
latent_matrix <- matrix(as.numeric(unlist(latent_matrix)), nrow=nrow(latent_matrix))
# Silhouette
fitPamBest <- pamk(latent_df_random, krange = 2:maxClus)
eval.array2d[, 'SI'] = fitPamBest$crit[2:maxClus]
# The rest of the clustering processes remain largely the same
# Determine the optimal number of clusters kBest
kBest <- which.max(eval.array2d[, 'SI']) + 1
# Perform clustering using kBest
fit <- pam(latent_df_random, kBest)
# Getting a list <sampleID, clusterID>
labels <- as.data.frame(as.factor(fit$cluster))
colnames(labels) <- c('cluster')
df.latent <- as.data.frame(latent_df_random)
df <- merge(df.latent, labels, by = 'row.names')
row.names(df) <- df$Row.names
df$Row.names <- NULL
df.out <- subset(df, select = cluster)
write.table(df.out, 'sampleId-cluster_pairs_latent.txt', quote = FALSE, sep = ',', row.names = TRUE)
# Visualizations remain the same:
plot_silhouette <- function(fit) {
sil_values <- silhouette(fit)
plot(sil_values, col = 1:kBest)
}
plot_silhouette(fit)
# Load necessary libraries
library(cluster)
library(factoextra)
# Assuming df is your data frame and you have already determined the number of clusters
# Let's say the number of clusters is stored in a variable named num_clusters
num_clusters <- 10  # or any other method you used to determine this
# Perform k-means clustering
kmeans_result <- kmeans(df[, -ncol(df)], centers = num_clusters)
# Access the total within-cluster sum of squares, which is the inertia
inertia <- kmeans_result$tot.withinss
# PCA on the latent space
pca_results <- prcomp(latent_df_random)
# Extract the first two principal components
pc1 <- pca_results$x[, 1]
pc2 <- pca_results$x[, 2]
plot_clusters <- function(pc1, pc2, clusters) {
plot(pc1, pc2, col = clusters, pch = 16,
xlab = "PC1", ylab = "PC2", main = "PCA Clusters",
cex = 1.5, las = 1)
legend("topright", legend = unique(clusters), fill = 1:max(clusters))
}
plot_clusters(pc1, pc2, fit$clustering)
# 4. RANDOM FOREST
# Print medoids with their values in the most relevant features
# Very important to intepretate the clusters!!
as_tibble(df[rownames(fit$medoids),])
# Supervised learning after feature selection
# Random forest with the clusters defined by PAM
formula <- formula("cluster~.")
model <- rf.10CV(df,formula)
saveRDS(model, "final_RF_bf_model.rsd")
sink('model_RF_bf.txt')
cat('Growth in medoids')
as_tibble(df[rownames(fit$medoids),]) %>% select(starts_with("Biomass"))
print(model)
sink()
mod <- model$model
pred <- model$pred
# fluxes importance independent by predicted class
plot.rf.var.importance.by.class.andMean.dotplot(mod,'flux','cluster',title='Most relevant fluxes',nBestFeatures=25)
# see predictions (and in which CV fold it was tested)
# Plot variable importance
#varImpPlot(model)
#plot.varImportance <- function(model) {
#  varImpPlot(model$finalModel)
#}
# Call the function to display the variable importance plot
#plot.varImportance(model)
# Call the function to generate variable importance plot
var_importance_plot <- plot.rf.var.importance.by.class.andMean.dotplot(mod, 'flux', 'cluster', title = 'Most relevant fluxes', nBestFeatures = 25)
# Load necessary libraries
library(randomForest)
library(caret)
library(e1071)
# Load your saved Random Forest model
model <- readRDS("final_RF_bf_model.rsd")
pca_data <- prcomp(df[, -ncol(df)], scale = TRUE)  # Perform PCA on your data
pca_df <- as.data.frame(pca_data$x)  # Convert PCA results to a dataframe
str(pca_df)
str(labels)
str(pred)
length(pca_df[,1])  # Number of rows in pca_df
length(labels$cluster)  # Assuming labels is a data frame with a column 'cluster'
length(pred)  # Number of predictions
# Combine PCA results and cluster labels
final_df <- cbind(pca_df, Cluster = labels, Predicted = as.numeric(pred))

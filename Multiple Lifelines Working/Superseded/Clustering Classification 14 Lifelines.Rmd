---
title: "R Notebook"
output: html_notebook
---

```{r}
library(caret)
library(tidyverse)
library(dplyr)
library(randomForest)
library(fpc) # pamk
library(cluster) # pam
library(ggplot2,quietly=TRUE)
suppressMessages(library(ellipse))
suppressMessages(library(RColorBrewer))
library(reshape2,quietly=TRUE) # Load the reshape2 package (for the melt() function)

source('robust.clustering.metagenomics.functions.r')


#####################################
### Function process.one.fluxes.file
#####################################
# a) To generate subjectID (suffix from file, between 'biomass_' and '.tsv')
# b) To generate sample ID: subjectID_[0*]seqNum
process.one.fluxes.file <- function(file){
  suffix = gsub('.csv$','',gsub('^fluxes_','',file))
  table=read.table(file,sep="\t",header=TRUE)
  table[,grep('time',colnames(table))] <- NULL # Remove repeated time columns (from different strains)
  id.subject=rep(suffix,nrow(table))
  id.num=seq(1,nrow(table),1)
  id.num=str_pad(id.num,5, pad = '0') # Add leading zeros, to easy sort
  id.vector=paste(id.subject, id.num,sep = '_')
  rownames(table)=id.vector
  return(table)
} # end-function process.one.fluxes.file

#####################################
### Function rbind.with.rownames
#####################################
rbind.with.rownames <- function(datalist) {
  require(plyr)
  temp <- rbind.fill(datalist)
  rownames(temp) <- unlist(lapply(datalist, row.names))
  return(temp)
} # end-function rbind.with.rownames

#####################################
### Function cluster names
#####################################
cluster_names <- function(df) {
  require(dplyr)
  means <- df %>% 
    select(starts_with("Biomass"), cluster) %>%
    group_by(cluster) %>%
    summarize_all(mean)
  signed <- c(as.character(means[which.max(rowSums(means[,2:ncol(means)])),]$cluster),
              as.character(means[which.min(rowSums(means[,2:ncol(means)])),]$cluster))
  df$cluster <- mapvalues(df$cluster, from = signed, to = c("growth", "stationary"))
  return(df)
} # end-function cluster names

#####################################
### Function rf.10CV
#####################################
# Build and evaluate (with 10 Cross-Validation) a model with Random Forest, using caret.
# Args:
#   df: data frame with all data (including predictors and class variable)
#   formula: object of class formula (i.e. class~ predictors (separated by '+'))
#   classVar: string/number of class variable in 'df'. Default: the last one.
#   ntree: number of trees in the forest
# Output:
#   finalModel: object with the model in randomForest format, for further evaluations or plots.
rf.10CV <- function(df,formula,classVar=length(df),ntree=1000){
  #require(doMC) # parallel processing; in Windows, maybe doParallel would work
  #registerDoMC(cores = 5)
  set.seed(123)
  train_control <- trainControl(method="cv", number=10, savePredictions = TRUE)
  model <- train(form=formula,data=df, trControl=train_control, method="rf", metric='Accuracy',
                 ntree=ntree, importance=TRUE, localImp=TRUE, na.action=na.omit,
                 allowParallel = T, ncores = 5, nodesize = 42)
  # Define output
  output <- list("model"=model$finalModel, "pred"=model$pred)
  # summarize results
  print(model)
  # plot model error
  print(plot(model))
  # Variable Importance
  pdf('varImportance_rf10CV.pdf')
  print(varImpPlot(model$finalModel))
  dev.off()
  print(plot(margin(model$finalModel,df[,classNum])))
  
  return(output)
}

####################################################
### plot.rf.var.importance.by.class.andMean.dotplot
####################################################
# Plot dotplot with variable importance mean over all classes
# Args:
#   model: random forest model already build
#   predVar: string of column ID with predictor/variables names values
#   classVar: string of class variable in 'df'
#   title: header of the plot
#   colorVector: vector of colors
#   nBestFeatures: number of top relevant features to show in the plot.
plot.rf.var.importance.by.class.andMean.dotplot <- function(model,predVar,classVar,title='',colorVector=NULL,nBestFeatures=NULL){
  library(reshape2)
  imp.df <- melt(importance(model)[,1:(length(model$classes)+1)])
  colnames(imp.df)=c(predVar,classVar,'value')
  # a.-Order features
  pred.order=names(sort(importance(model)[,'MeanDecreaseAccuracy'])) # My order according to global MeandDecreaseAccuracy
  imp.df[,predVar] <- factor(imp.df[,predVar], levels = pred.order)
  class.names=levels(imp.df[,classVar])
  levels(imp.df[,classVar]) <- c(class.names[1:(length(class.names)-1)],"MEAN")
  imp.df[,classVar] <- factor(imp.df[,classVar])
  # b.- Subset features to show
  if(!is.null(nBestFeatures)){
    imp.df=subset(imp.df,subset=(imp.df[,predVar] %in% tail(pred.order,n=nBestFeatures)))
  }
  p <- ggplot(imp.df, aes_string(x = 'value', y = predVar, group = predVar, colour = classVar)) +
    geom_segment(aes_string(yend=predVar), xend=0, colour="grey50") +
    geom_point( size = 1) +
    theme_bw() +
    facet_grid(reformulate(classVar)) +
    theme(panel.grid.major.y = element_blank()) +
    #theme(text = element_text(size=16)) +
    xlab(paste(predVar," importance",sep='')) +
    theme(axis.text.x = element_text(angle = 270, hjust = 1)) +
    theme(legend.position="none") +
    ggtitle(title)
  if(!is.null(colorVector)){
    p +  scale_color_manual(values=colorVector)
  }else{
    p
  }
  return(p)
}

```

```{r}
# 1. READ DATA
# Read fluxes tables from MMODES
#file <- list.files(pattern="^fluxes\\_.*\\.tsv$")
# Add subject and sample ID per table
#tables.list=lapply(file, process.one.fluxes.file)
# Concatenate all fluxes data.frames (one per subject time series) in a unique one
#table.all <- rbind.with.rownames(tables.list)
# Save
#fluxes_raw  <- table.all
#saveRDS(fluxes, "fluxes.rds")
#write.table(fluxes,file='allTimeSeries_fluxes.tsv',sep='\t',quote=FALSE)
#fluxes_raw <- readRDS("fluxes.rds")

# 1. READ DATA
# Specify the path to your CSV file
data_file <- "D:/Desktop/Work/dFBA_Textbook_Model_Reaction_Fluxes.csv"

# Read the CSV file with tab separator and skip the first column (time)
table.all_graph <- read.csv(data_file, header = TRUE, sep = "\t", check.names = FALSE)
table.all <- read.csv(data_file, header = TRUE, sep = "\t", check.names = FALSE)[,-1]

# Optionally, you can add subject and sample ID if needed.
# For example:
# table.all$subjectID <- "your_subject_id"
# table.all$sampleID <- paste0(table.all$subjectID, "_", rownames(table.all))

# Save the data if necessary
# write.table(table.all, file = 'allTimeSeries_fluxes.tsv', sep = '\t', quote = FALSE, row.names = FALSE)

# Assign the loaded data to fluxes_raw
fluxes_graph <-table.all_graph
fluxes_raw <- table.all

# Save it in RDS format if needed
# saveRDS(fluxes_raw, "fluxes.rds")


# 1.2. FEATURE SELECTION
# Check columns with NA values
# fluxes.isna=colnames(fluxes)[colSums(is.na(fluxes)) > 0]
# NA values are generated if some Perturbations are missing in some simulation.
# We can afford to remove the rows of these simulations (generally 1 or 0 in the dataset).
fluxes <- fluxes_raw[complete.cases(fluxes_raw),]
# Check fluxes with constant values (sd=0)
output_sd=apply(fluxes, MARGIN=2, sd)
fluxes.sd0=colnames(fluxes)[output_sd==0]
# Remove those fluxes
if(length(fluxes.sd0)>0){
  fluxes = fluxes[, -which(names(fluxes) %in% fluxes.sd0)]
}
# Remove first all of the transporters and exchanges
# It does not remove knowledge, but interpretations of the inner metabolism is possible
fluxes <- fluxes %>%
  select(-starts_with("EX"), -starts_with("Ex"), -contains("tex_"), -contains("tpp_"), -contains("abcpp"))
# Some columns shares identical values across rows. That is because reactions are coupled,
# belonging to the same pathway
fluxes <- fluxes[,-which(duplicated(t(fluxes)) == TRUE)]
cat("Predictos variables reduced sized from", ncol(fluxes_raw), "to", ncol(fluxes), 
    "by feature selection.")

# normalization
library(scales)
fluxes.rescale <- apply(fluxes[,1:ncol(fluxes)-1], MARGIN = 2, rescale, to=c(-1,1))
```

```{r}
library(ggplot2)

# Plot "Biomass_Ecoli_core" over Time
ggplot(fluxes, aes(x=`Time (s)`, y=Biomass_Ecoli_core)) + 
    geom_line() + 
    labs(title="Biomass_Ecoli_core over Time")


```


```{r}
library(pheatmap)

# Create a smaller dataset for visualization (using the first 100 rows for example)
small_data <- fluxes[1:100,]

pheatmap(as.matrix(small_data))

```
```{r}
# Plotting for a specific feature
ggplot(fluxes, aes(x=`Time (s)`, y=Biomass_Ecoli_core)) + 
    geom_line() + 
    geom_smooth(method="loess") +
    labs(title="Noise Visualization for YourFeatureColumnName")

```


```{r}
acf(fluxes$Biomass_Ecoli_core, main="ACF for Biomass Reaction")

```

```{r}
# 2. RANDOM SHUFFLE
iniSeed <- 1234
set.seed(iniSeed)
selected <- sample(nrow(fluxes.rescale))
fluxes.rescale.random <- fluxes.rescale[selected,] # for clustering
fluxes.random <- fluxes[selected,] # for random forest, without normalization
```


```{r}
# 3. CLUSTERING
maxClus=10
eval.array2d <- array(0,dim=c(maxClus-1,3),dimnames=list(as.character(seq(2,maxClus,1)), list('SI','PS','Jaccard')))
# Silhouette
fitPamBest <- pamk(fluxes.rescale.random,krange=2:maxClus)
save(fitPamBest,file='fitPamBest.Rdata')
eval.array2d[,'SI']=fitPamBest$crit[2:maxClus]

# Prediction Strength
out.pred.str <- prediction.strength(fluxes.rescale.random, Gmin=2, Gmax=maxClus, M=50, clustermethod=claraCBI, classification="centroid")
eval.array2d[,'PS']=out.pred.str$mean.pred[2:maxClus]
# Jaccard
for(k in 2:maxClus){
  cf <- clusterboot(fluxes.rescale.random,B=100,bootmethod="boot",clustermethod=claraCBI,k=k,seed=iniSeed,count=FALSE)
  #print(mean(cf$bootmean))
  eval.array2d[as.character(k),'Jaccard'] <- mean(cf$bootmean)
}
```


```{r}
library(cluster)

# Calculate average silhouette width for a range of k values
max_k <- 10 # or whatever maximum k you want to consider
sil_widths <- sapply(2:max_k, function(k) {
  pam_fit <- pam(fluxes.rescale.random, diss = FALSE, k)
  sil_values <- silhouette(pam_fit)
  mean(sil_values[, "sil_width"])  # Get the mean silhouette width directly from the matrix
})


# Determine the optimal number of clusters kBest
kBest <- which.max(sil_widths) + 1
# simplified this because error with robust.clustering.decision

# Perform clustering using kBest
fit <- pam(fluxes.rescale.random,kBest)

# Getting a list <sampleID,clusterID>
labels <-  as.data.frame(as.factor(fit$cluster))
colnames(labels) <- c('cluster')
df.fluxes <- as.data.frame(fluxes.random)
df <- merge(df.fluxes, labels, by = 'row.names')
row.names(df) <- df$Row.names
df$Row.names <- NULL
df.out <- subset(df, select = cluster)
write.table(df.out, 'sampleId-cluster_pairs_fluxes.txt', quote = FALSE, sep = ',', row.names = TRUE)
rm(df.fluxes, df.out)

plot_silhouette <- function(fit) {
  # Compute silhouette values
  sil_values <- silhouette(fit)
  
  # Plot the silhouette values
  plot(sil_values, col = 1:kBest)  # Use kBest here
}

# Call the function
plot_silhouette(fit)


```
```{r}
# Compute PCA on the scaled data
pca_results <- prcomp(fluxes.rescale.random)

# Extract the first two principal components
pc1 <- pca_results$x[,1]
pc2 <- pca_results$x[,2]

# Plot the clusters using the first two principal components
plot_clusters <- function(pc1, pc2, clusters) {
  # Create a scatter plot of the first two principal components
  plot(pc1, pc2, col = clusters, pch = 16,
       xlab = "PC1", ylab = "PC2", main = "PCA Clusters",
       cex = 1.5, las = 1)
  
  # Add a legend
  legend("topright", legend = unique(clusters), fill = 1:max(clusters))
}

# Call the function
plot_clusters(pc1, pc2, fit$clustering)

```


```{r}
# 4. RANDOM FOREST
# Print medoids with their values in the most relevant features
# Very important to intepretate the clusters!!
as_tibble(df[rownames(fit$medoids),])

# Supervised learning after feature selection
# Random forest with the clusters defined by PAM
formula <- formula("cluster~.")
model <- rf.10CV(df,formula)
saveRDS(model, "final_RF_bf_model.rsd")
sink('model_RF_bf.txt')
cat('Growth in medoids')
as_tibble(df[rownames(fit$medoids),]) %>% select(starts_with("Biomass"))
print(model)
sink()
mod <- model$model
pred <- model$pred
# fluxes importance independent by predicted class
plot.rf.var.importance.by.class.andMean.dotplot(mod,'flux','cluster',title='Most relevant fluxes',nBestFeatures=25)
# see predictions (and in which CV fold it was tested)

# Plot variable importance
#varImpPlot(model)

#plot.varImportance <- function(model) {
#  varImpPlot(model$finalModel)
#}

# Call the function to display the variable importance plot
#plot.varImportance(model)

# Call the function to generate variable importance plot
var_importance_plot <- plot.rf.var.importance.by.class.andMean.dotplot(mod, 'flux', 'cluster', title = 'Most relevant fluxes', nBestFeatures = 25)



```

```{r}
# Extract variable importance
var_importance <- varImp(model$model, scale = FALSE)

# Plot variable importance
plot(var_importance)
```
```{r}
# For autoencoder latent space representation 
# 1. READ LATENT SPACE DATA

# Specify the path to your latent space CSV file
latent_data_file <- "D:/Desktop/Work/Multiple Lifelines/latent_data_14lifelines.csv"

# Read the CSV file
latent_df <- read.csv(latent_data_file, header = TRUE, sep = ",", check.names = FALSE)

# 2. RANDOM SHUFFLE
# This step ensures that you're working with a random arrangement of your samples, which can be useful for subsequent analyses.

iniSeed <- 1234
set.seed(iniSeed)
selected <- sample(nrow(latent_df))
latent_df_random <- latent_df[selected,]


```

```{r}
# 3. CLUSTERING
maxClus <- 10
eval.array2d <- array(0, dim = c(maxClus - 1, 3),
                      dimnames = list(as.character(seq(2, maxClus, 1)), c('SI', 'PS', 'Jaccard')))

# Convert dataframe to matrix and ensure all entries are numeric
latent_matrix <- as.matrix(latent_df_random)
latent_matrix <- matrix(as.numeric(unlist(latent_matrix)), nrow=nrow(latent_matrix))

# Silhouette
fitPamBest <- pamk(latent_df_random, krange = 2:maxClus)
eval.array2d[, 'SI'] = fitPamBest$crit[2:maxClus]

# The rest of the clustering processes remain largely the same

# Determine the optimal number of clusters kBest
kBest <- which.max(eval.array2d[, 'SI']) + 1

# Perform clustering using kBest
fit <- pam(latent_df_random, kBest)

# Getting a list <sampleID, clusterID>
labels <- as.data.frame(as.factor(fit$cluster))
colnames(labels) <- c('cluster')
df.latent <- as.data.frame(latent_df_random)
df <- merge(df.latent, labels, by = 'row.names')
row.names(df) <- df$Row.names
df$Row.names <- NULL
df.out <- subset(df, select = cluster)
write.table(df.out, 'sampleId-cluster_pairs_latent.txt', quote = FALSE, sep = ',', row.names = TRUE)

# Visualizations remain the same:

plot_silhouette <- function(fit) {
  sil_values <- silhouette(fit)
  plot(sil_values, col = 1:kBest)
}

plot_silhouette(fit)

```
```{r}
# Load necessary libraries
library(cluster)
library(factoextra)

# Assuming df is your data frame and you have already determined the number of clusters
# Let's say the number of clusters is stored in a variable named num_clusters

num_clusters <- 10  # or any other method you used to determine this

# Perform k-means clustering
kmeans_result <- kmeans(df[, -ncol(df)], centers = num_clusters)

# Access the total within-cluster sum of squares, which is the inertia
inertia <- kmeans_result$tot.withinss

```

```{r}
# PCA on the latent space
pca_results <- prcomp(latent_df_random)

# Extract the first two principal components
pc1 <- pca_results$x[, 1]
pc2 <- pca_results$x[, 2]

plot_clusters <- function(pc1, pc2, clusters) {
  plot(pc1, pc2, col = clusters, pch = 16,
       xlab = "PC1", ylab = "PC2", main = "PCA Clusters",
       cex = 1.5, las = 1)
  legend("topright", legend = unique(clusters), fill = 1:max(clusters))
}

plot_clusters(pc1, pc2, fit$clustering)
```
```{r}
# 4. RANDOM FOREST
# Print medoids with their values in the most relevant features
# Very important to intepretate the clusters!!
as_tibble(df[rownames(fit$medoids),])

# Supervised learning after feature selection
# Random forest with the clusters defined by PAM
formula <- formula("cluster~.")
model <- rf.10CV(df,formula)
saveRDS(model, "final_RF_bf_model.rsd")
sink('model_RF_bf.txt')
cat('Growth in medoids')
as_tibble(df[rownames(fit$medoids),]) %>% select(starts_with("Biomass"))
print(model)
sink()
mod <- model$model
pred <- model$pred
# fluxes importance independent by predicted class
plot.rf.var.importance.by.class.andMean.dotplot(mod,'flux','cluster',title='Most relevant fluxes',nBestFeatures=25)
# see predictions (and in which CV fold it was tested)

# Plot variable importance
#varImpPlot(model)

#plot.varImportance <- function(model) {
#  varImpPlot(model$finalModel)
#}

# Call the function to display the variable importance plot
#plot.varImportance(model)

# Call the function to generate variable importance plot
var_importance_plot <- plot.rf.var.importance.by.class.andMean.dotplot(mod, 'flux', 'cluster', title = 'Most relevant fluxes', nBestFeatures = 25)

```
```{r}
# Load necessary libraries
library(randomForest)
library(caret)
library(e1071)

# Load your saved Random Forest model
model <- readRDS("final_RF_bf_model.rsd")

pca_data <- prcomp(df[, -ncol(df)], scale = TRUE)  # Perform PCA on your data
pca_df <- as.data.frame(pca_data$x)  # Convert PCA results to a dataframe

str(pca_df)
str(labels)
str(pred)

length(pca_df[,1])  # Number of rows in pca_df
length(labels$cluster)  # Assuming labels is a data frame with a column 'cluster'
length(pred)  # Number of predictions

# Combine PCA results and cluster labels
final_df <- cbind(pca_df, Cluster = labels, Predicted = as.numeric(pred))


# Plot the PCA results with color-coded clusters and predictions
library(ggplot2)
ggplot(final_df, aes(x = PC1, y = PC2, color = factor(Cluster))) +
  geom_point() +
  labs(title = "PCA Plot with Clusters") +
  theme_minimal()

# Alternatively, you can also visualize the PCA plot with color-coded predictions
ggplot(final_df, aes(x = PC1, y = PC2, color = factor(Predicted))) +
  geom_point() +
  labs(title = "PCA Plot with Predictions") +
  theme_minimal()
```

```{r}
# Extract variable importance
var_importance <- varImp(model$model, scale = FALSE)

# Plot variable importance
plot(var_importance)
```






